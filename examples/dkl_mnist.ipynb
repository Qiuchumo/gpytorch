{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gpytorch\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "gpytorch.functions.use_toeplitz = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataset = datasets.MNIST('/tmp', train=True, download=True,\n",
    "                               transform=transforms.Compose([\n",
    "                                   transforms.ToTensor(),\n",
    "                                   transforms.Normalize((0.1307,), (0.3081,))\n",
    "                               ]))\n",
    "test_dataset = datasets.MNIST('/tmp', train=False, download=True,\n",
    "                              transform=transforms.Compose([\n",
    "                                  transforms.ToTensor(),\n",
    "                                  transforms.Normalize((0.1307,), (0.3081,))\n",
    "                              ]))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=256, shuffle=True, pin_memory=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=256, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the feature extractor for our deep kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class LeNetFeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNetFeatureExtractor, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=5, padding=2)\n",
    "        self.norm1 = nn.BatchNorm2d(16)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, padding=2)\n",
    "        self.norm2 = nn.BatchNorm2d(32)\n",
    "        self.fc3 = nn.Linear(32 * 7 * 7, 64)\n",
    "        self.norm3 = nn.BatchNorm1d(64)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.norm1(self.conv1(x))), 2)\n",
    "        x = F.max_pool2d(F.relu(self.norm2(self.conv2(x))), 2)\n",
    "        x = x.view(-1, 32 * 7 * 7)\n",
    "        x = F.relu(self.norm3(self.fc3(x)))\n",
    "        return x\n",
    "    \n",
    "feature_extractor = LeNetFeatureExtractor().cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretrain the feature extractor a bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1\tLoss: 0.145056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gpleiss/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:26: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 0.0398, Accuracy: 9866/10000 (98.660%)\n",
      "Train Epoch: 2\tLoss: 0.035001\n",
      "Test set: Average loss: 0.0380, Accuracy: 9884/10000 (98.840%)\n",
      "Train Epoch: 3\tLoss: 0.023860\n",
      "Test set: Average loss: 0.0265, Accuracy: 9904/10000 (99.040%)\n"
     ]
    }
   ],
   "source": [
    "classifier = nn.Linear(64, 10).cuda()\n",
    "params = list(feature_extractor.parameters()) + list(classifier.parameters())\n",
    "optimizer = torch.optim.SGD(params, lr=0.1, momentum=0.9)\n",
    "\n",
    "def pretrain(epoch):\n",
    "    feature_extractor.train()\n",
    "    train_loss = 0.\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        optimizer.zero_grad()\n",
    "        features = feature_extractor(data)\n",
    "        output = F.log_softmax(classifier(features), 1)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.data[0] * len(data)\n",
    "    print('Train Epoch: %d\\tLoss: %.6f' % (epoch, train_loss / len(train_dataset)))\n",
    "\n",
    "def pretest():\n",
    "    feature_extractor.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        features = feature_extractor(data)\n",
    "        output = F.log_softmax(classifier(features), 1)\n",
    "        test_loss += F.nll_loss(output, target, size_average=False).data[0] # sum up batch loss\n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.3f}%)'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "n_epochs = 3\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    pretrain(epoch)\n",
    "    pretest()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the deep kernel GP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DKLModel(gpytorch.GPModel):\n",
    "    def __init__(self, feature_extractor, n_features=64, n_classes=10, grid_bounds=(-10., 10.)):\n",
    "        likelihood = gpytorch.likelihoods.SoftmaxLikelihood(n_features=n_features, n_classes=n_classes)\n",
    "        super(DKLModel, self).__init__(likelihood)\n",
    "        \n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.latent_functions = LatentFunctions(n_features=n_features, grid_bounds=grid_bounds)\n",
    "        self.grid_bounds = grid_bounds\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = self.feature_extractor(x)\n",
    "        # Scale to fit insid egrid bounds\n",
    "        features = gpytorch.utils.scale_to_bounds(features, self.grid_bounds[0], self.grid_bounds[1])\n",
    "        res = self.latent_functions(features.unsqueeze(-1))\n",
    "        return res\n",
    "    \n",
    "    \n",
    "class LatentFunctions(gpytorch.AdditiveGridInducingPointModule):\n",
    "    def __init__(self, n_features, grid_bounds, grid_size=128):\n",
    "        super(LatentFunctions, self).__init__(grid_size=grid_size, grid_bounds=[grid_bounds],\n",
    "                                             n_components=n_features, mixing_params=False, sum_output=False)\n",
    "        cov_module = gpytorch.kernels.RBFKernel()\n",
    "        cov_module.initialize(log_lengthscale=0)\n",
    "        self.cov_module = cov_module\n",
    "        \n",
    "    def forward(self, x):\n",
    "        mean = Variable(x.data.new(len(x)).zero_())\n",
    "        covar = self.cov_module(x)\n",
    "        return gpytorch.random_variables.GaussianRandomVariable(mean, covar)\n",
    "    \n",
    "    \n",
    "model = DKLModel(feature_extractor).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [001/030], Loss: 42.384556\n",
      "Train Epoch: 1 [002/030], Loss: 59.947166\n",
      "Train Epoch: 1 [003/030], Loss: 46.958019\n",
      "Train Epoch: 1 [004/030], Loss: 37.468971\n",
      "Train Epoch: 1 [005/030], Loss: 38.515999\n",
      "Train Epoch: 1 [006/030], Loss: 33.736656\n",
      "Train Epoch: 1 [007/030], Loss: 61.322174\n",
      "Train Epoch: 1 [008/030], Loss: 27.759411\n",
      "Train Epoch: 1 [009/030], Loss: 21.316530\n",
      "Train Epoch: 1 [010/030], Loss: 24.071991\n",
      "Train Epoch: 1 [011/030], Loss: 21.866581\n",
      "Train Epoch: 1 [012/030], Loss: 16.668083\n",
      "Train Epoch: 1 [013/030], Loss: 19.503393\n",
      "Train Epoch: 1 [014/030], Loss: 13.266150\n",
      "Train Epoch: 1 [015/030], Loss: 12.656313\n",
      "Train Epoch: 1 [016/030], Loss: 14.961938\n",
      "Train Epoch: 1 [017/030], Loss: 11.993261\n",
      "Train Epoch: 1 [018/030], Loss: 9.440063\n",
      "Train Epoch: 1 [019/030], Loss: 12.061893\n",
      "Train Epoch: 1 [020/030], Loss: 9.291729\n",
      "Train Epoch: 1 [021/030], Loss: 7.797052\n",
      "Train Epoch: 1 [022/030], Loss: 6.539400\n",
      "Train Epoch: 1 [023/030], Loss: 7.787019\n",
      "Train Epoch: 1 [024/030], Loss: 6.343824\n",
      "Train Epoch: 1 [025/030], Loss: 6.229316\n",
      "Train Epoch: 1 [026/030], Loss: 5.311652\n",
      "Train Epoch: 1 [027/030], Loss: 17.884089\n",
      "Train Epoch: 1 [028/030], Loss: 7.176664\n",
      "Train Epoch: 1 [029/030], Loss: 4.811298\n",
      "Train Epoch: 1 [030/030], Loss: 8.995207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gpleiss/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:24: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 0.0000, Accuracy: 8902/10000 (89.020%)\n",
      "Train Epoch: 2 [001/030], Loss: 5.266603\n",
      "Train Epoch: 2 [002/030], Loss: 3.753875\n",
      "Train Epoch: 2 [003/030], Loss: 3.828629\n",
      "Train Epoch: 2 [004/030], Loss: 3.631824\n",
      "Train Epoch: 2 [005/030], Loss: 4.894492\n",
      "Train Epoch: 2 [006/030], Loss: 2.812186\n",
      "Train Epoch: 2 [007/030], Loss: 6.411754\n",
      "Train Epoch: 2 [008/030], Loss: 2.969193\n",
      "Train Epoch: 2 [009/030], Loss: 3.277704\n",
      "Train Epoch: 2 [010/030], Loss: 3.264744\n",
      "Train Epoch: 2 [011/030], Loss: 2.062227\n",
      "Train Epoch: 2 [012/030], Loss: 4.527439\n",
      "Train Epoch: 2 [013/030], Loss: 3.147879\n",
      "Train Epoch: 2 [014/030], Loss: 2.526008\n",
      "Train Epoch: 2 [015/030], Loss: 2.639375\n",
      "Train Epoch: 2 [016/030], Loss: 2.211667\n",
      "Train Epoch: 2 [017/030], Loss: 2.150765\n",
      "Train Epoch: 2 [018/030], Loss: 2.678360\n",
      "Train Epoch: 2 [019/030], Loss: 2.035064\n",
      "Train Epoch: 2 [020/030], Loss: 1.618116\n",
      "Train Epoch: 2 [021/030], Loss: 1.722281\n",
      "Train Epoch: 2 [022/030], Loss: 2.044909\n",
      "Train Epoch: 2 [023/030], Loss: 1.608499\n",
      "Train Epoch: 2 [024/030], Loss: 1.474004\n",
      "Train Epoch: 2 [025/030], Loss: 1.362737\n",
      "Train Epoch: 2 [026/030], Loss: 1.577304\n",
      "Train Epoch: 2 [027/030], Loss: 1.600409\n",
      "Train Epoch: 2 [028/030], Loss: 1.352444\n",
      "Train Epoch: 2 [029/030], Loss: 1.366466\n",
      "Train Epoch: 2 [030/030], Loss: 1.149343\n",
      "Test set: Average loss: 0.0000, Accuracy: 9767/10000 (97.670%)\n",
      "Train Epoch: 3 [001/030], Loss: 1.146372\n",
      "Train Epoch: 3 [002/030], Loss: 1.189398\n",
      "Train Epoch: 3 [003/030], Loss: 1.293468\n",
      "Train Epoch: 3 [004/030], Loss: 1.183195\n",
      "Train Epoch: 3 [005/030], Loss: 1.006704\n",
      "Train Epoch: 3 [006/030], Loss: 0.958029\n",
      "Train Epoch: 3 [007/030], Loss: 1.095747\n",
      "Train Epoch: 3 [008/030], Loss: 1.156485\n",
      "Train Epoch: 3 [009/030], Loss: 0.598687\n",
      "Train Epoch: 3 [010/030], Loss: 0.752238\n",
      "Train Epoch: 3 [011/030], Loss: 0.949355\n",
      "Train Epoch: 3 [012/030], Loss: 0.654763\n",
      "Train Epoch: 3 [013/030], Loss: 0.764778\n",
      "Train Epoch: 3 [014/030], Loss: 0.630243\n",
      "Train Epoch: 3 [015/030], Loss: 0.479384\n",
      "Train Epoch: 3 [016/030], Loss: 0.928604\n",
      "Train Epoch: 3 [017/030], Loss: 0.631454\n",
      "Train Epoch: 3 [018/030], Loss: 0.767125\n",
      "Train Epoch: 3 [019/030], Loss: 0.635436\n",
      "Train Epoch: 3 [020/030], Loss: 0.638236\n",
      "Train Epoch: 3 [021/030], Loss: 0.891429\n",
      "Train Epoch: 3 [022/030], Loss: 0.398548\n",
      "Train Epoch: 3 [023/030], Loss: 0.830029\n",
      "Train Epoch: 3 [024/030], Loss: 0.488594\n",
      "Train Epoch: 3 [025/030], Loss: 0.420996\n",
      "Train Epoch: 3 [026/030], Loss: 1.078277\n",
      "Train Epoch: 3 [027/030], Loss: 0.465087\n",
      "Train Epoch: 3 [028/030], Loss: 0.385862\n",
      "Train Epoch: 3 [029/030], Loss: 0.380000\n",
      "Train Epoch: 3 [030/030], Loss: 0.347398\n",
      "Test set: Average loss: 0.0000, Accuracy: 9838/10000 (98.380%)\n",
      "Train Epoch: 4 [001/030], Loss: 0.362220\n",
      "Train Epoch: 4 [002/030], Loss: 0.355309\n",
      "Train Epoch: 4 [003/030], Loss: 0.399802\n",
      "Train Epoch: 4 [004/030], Loss: 0.280948\n",
      "Train Epoch: 4 [005/030], Loss: 0.276637\n",
      "Train Epoch: 4 [006/030], Loss: 0.386159\n",
      "Train Epoch: 4 [007/030], Loss: 0.249692\n",
      "Train Epoch: 4 [008/030], Loss: 0.333114\n",
      "Train Epoch: 4 [009/030], Loss: 0.263904\n",
      "Train Epoch: 4 [010/030], Loss: 0.245761\n",
      "Train Epoch: 4 [011/030], Loss: 0.430808\n",
      "Train Epoch: 4 [012/030], Loss: 0.227885\n",
      "Train Epoch: 4 [013/030], Loss: 0.277148\n",
      "Train Epoch: 4 [014/030], Loss: 0.273563\n",
      "Train Epoch: 4 [015/030], Loss: 0.220822\n",
      "Train Epoch: 4 [016/030], Loss: 0.187021\n",
      "Train Epoch: 4 [017/030], Loss: 0.169309\n",
      "Train Epoch: 4 [018/030], Loss: 0.167686\n",
      "Train Epoch: 4 [019/030], Loss: 0.185530\n",
      "Train Epoch: 4 [020/030], Loss: 0.193656\n",
      "Train Epoch: 4 [021/030], Loss: 0.228601\n",
      "Train Epoch: 4 [022/030], Loss: 0.302610\n",
      "Train Epoch: 4 [023/030], Loss: 0.179828\n",
      "Train Epoch: 4 [024/030], Loss: 1.075109\n",
      "Train Epoch: 4 [025/030], Loss: 0.155114\n",
      "Train Epoch: 4 [026/030], Loss: 0.168806\n",
      "Train Epoch: 4 [027/030], Loss: 0.145662\n",
      "Train Epoch: 4 [028/030], Loss: 0.121304\n",
      "Train Epoch: 4 [029/030], Loss: 0.293936\n",
      "Train Epoch: 4 [030/030], Loss: 0.189614\n",
      "Test set: Average loss: 0.0000, Accuracy: 9861/10000 (98.610%)\n",
      "Train Epoch: 5 [001/030], Loss: 0.179582\n",
      "Train Epoch: 5 [002/030], Loss: 0.111387\n",
      "Train Epoch: 5 [003/030], Loss: 0.117557\n",
      "Train Epoch: 5 [004/030], Loss: 0.138374\n",
      "Train Epoch: 5 [005/030], Loss: 0.130191\n",
      "Train Epoch: 5 [006/030], Loss: 0.141665\n",
      "Train Epoch: 5 [007/030], Loss: 0.115919\n",
      "Train Epoch: 5 [008/030], Loss: 0.093083\n",
      "Train Epoch: 5 [009/030], Loss: 0.149931\n",
      "Train Epoch: 5 [010/030], Loss: 0.081701\n",
      "Train Epoch: 5 [011/030], Loss: 0.109489\n",
      "Train Epoch: 5 [012/030], Loss: 0.107923\n",
      "Train Epoch: 5 [013/030], Loss: 0.084794\n",
      "Train Epoch: 5 [014/030], Loss: 0.117715\n",
      "Train Epoch: 5 [015/030], Loss: 0.077489\n",
      "Train Epoch: 5 [016/030], Loss: 0.094844\n",
      "Train Epoch: 5 [017/030], Loss: 0.118882\n",
      "Train Epoch: 5 [018/030], Loss: 0.095214\n",
      "Train Epoch: 5 [019/030], Loss: 0.128748\n",
      "Train Epoch: 5 [020/030], Loss: 0.627851\n",
      "Train Epoch: 5 [021/030], Loss: 0.094833\n",
      "Train Epoch: 5 [022/030], Loss: 0.211312\n",
      "Train Epoch: 5 [023/030], Loss: 0.082448\n",
      "Train Epoch: 5 [024/030], Loss: 0.087105\n",
      "Train Epoch: 5 [025/030], Loss: 0.083124\n",
      "Train Epoch: 5 [026/030], Loss: 0.064056\n",
      "Train Epoch: 5 [027/030], Loss: 0.072156\n",
      "Train Epoch: 5 [028/030], Loss: 0.210987\n",
      "Train Epoch: 5 [029/030], Loss: 0.063367\n",
      "Train Epoch: 5 [030/030], Loss: 0.047947\n",
      "Test set: Average loss: 0.0000, Accuracy: 9873/10000 (98.730%)\n",
      "Train Epoch: 6 [001/030], Loss: 0.058401\n",
      "Train Epoch: 6 [002/030], Loss: 0.098196\n",
      "Train Epoch: 6 [003/030], Loss: 0.045882\n",
      "Train Epoch: 6 [004/030], Loss: 0.064543\n",
      "Train Epoch: 6 [005/030], Loss: 0.018437\n",
      "Train Epoch: 6 [006/030], Loss: 0.044522\n",
      "Train Epoch: 6 [007/030], Loss: 0.070363\n",
      "Train Epoch: 6 [008/030], Loss: 0.089776\n",
      "Train Epoch: 6 [009/030], Loss: 0.070492\n",
      "Train Epoch: 6 [010/030], Loss: 0.062397\n",
      "Train Epoch: 6 [011/030], Loss: 0.094381\n",
      "Train Epoch: 6 [012/030], Loss: 0.071214\n",
      "Train Epoch: 6 [013/030], Loss: 0.043923\n",
      "Train Epoch: 6 [014/030], Loss: 0.046107\n",
      "Train Epoch: 6 [015/030], Loss: 0.073759\n",
      "Train Epoch: 6 [016/030], Loss: 0.110195\n",
      "Train Epoch: 6 [017/030], Loss: 0.035374\n",
      "Train Epoch: 6 [018/030], Loss: 0.089798\n",
      "Train Epoch: 6 [019/030], Loss: 0.023806\n",
      "Train Epoch: 6 [020/030], Loss: 0.185190\n",
      "Train Epoch: 6 [021/030], Loss: 0.043874\n",
      "Train Epoch: 6 [022/030], Loss: 0.027481\n",
      "Train Epoch: 6 [023/030], Loss: 0.037381\n",
      "Train Epoch: 6 [024/030], Loss: 0.051594\n",
      "Train Epoch: 6 [025/030], Loss: 0.040055\n",
      "Train Epoch: 6 [026/030], Loss: 0.034550\n",
      "Train Epoch: 6 [027/030], Loss: 0.023309\n",
      "Train Epoch: 6 [028/030], Loss: 0.077561\n",
      "Train Epoch: 6 [029/030], Loss: 0.024449\n",
      "Train Epoch: 6 [030/030], Loss: 0.025225\n",
      "Test set: Average loss: 0.0000, Accuracy: 9885/10000 (98.850%)\n",
      "Train Epoch: 7 [001/030], Loss: 0.049960\n",
      "Train Epoch: 7 [002/030], Loss: 0.018316\n",
      "Train Epoch: 7 [003/030], Loss: 0.023042\n",
      "Train Epoch: 7 [004/030], Loss: 0.016078\n",
      "Train Epoch: 7 [005/030], Loss: 0.022527\n",
      "Train Epoch: 7 [006/030], Loss: 0.026305\n",
      "Train Epoch: 7 [007/030], Loss: 0.014573\n",
      "Train Epoch: 7 [008/030], Loss: 0.023023\n",
      "Train Epoch: 7 [009/030], Loss: 0.052599\n",
      "Train Epoch: 7 [010/030], Loss: 0.037708\n",
      "Train Epoch: 7 [011/030], Loss: 0.011338\n",
      "Train Epoch: 7 [012/030], Loss: 0.014629\n",
      "Train Epoch: 7 [013/030], Loss: 0.050769\n",
      "Train Epoch: 7 [014/030], Loss: -0.002002\n",
      "Train Epoch: 7 [015/030], Loss: 0.010397\n",
      "Train Epoch: 7 [016/030], Loss: 0.014912\n",
      "Train Epoch: 7 [017/030], Loss: 0.027486\n",
      "Train Epoch: 7 [018/030], Loss: 0.052202\n",
      "Train Epoch: 7 [019/030], Loss: 0.024242\n",
      "Train Epoch: 7 [020/030], Loss: 0.021764\n",
      "Train Epoch: 7 [021/030], Loss: 0.033014\n",
      "Train Epoch: 7 [022/030], Loss: 0.056109\n",
      "Train Epoch: 7 [023/030], Loss: 0.016055\n",
      "Train Epoch: 7 [024/030], Loss: 0.028145\n",
      "Train Epoch: 7 [025/030], Loss: 0.183708\n",
      "Train Epoch: 7 [026/030], Loss: 0.408257\n",
      "Train Epoch: 7 [027/030], Loss: 0.030367\n",
      "Train Epoch: 7 [028/030], Loss: 0.008818\n",
      "Train Epoch: 7 [029/030], Loss: -0.003739\n",
      "Train Epoch: 7 [030/030], Loss: 0.073334\n",
      "Test set: Average loss: 0.0000, Accuracy: 9890/10000 (98.900%)\n",
      "Train Epoch: 8 [001/030], Loss: 0.237932\n",
      "Train Epoch: 8 [002/030], Loss: -0.014343\n",
      "Train Epoch: 8 [003/030], Loss: 0.013939\n",
      "Train Epoch: 8 [004/030], Loss: 0.009050\n",
      "Train Epoch: 8 [005/030], Loss: 0.003491\n",
      "Train Epoch: 8 [006/030], Loss: -0.005021\n",
      "Train Epoch: 8 [007/030], Loss: 0.015094\n",
      "Train Epoch: 8 [008/030], Loss: 0.006356\n",
      "Train Epoch: 8 [009/030], Loss: 0.024517\n",
      "Train Epoch: 8 [010/030], Loss: 0.010084\n",
      "Train Epoch: 8 [011/030], Loss: -0.014782\n",
      "Train Epoch: 8 [012/030], Loss: 0.038511\n",
      "Train Epoch: 8 [013/030], Loss: 0.022852\n",
      "Train Epoch: 8 [014/030], Loss: 0.013903\n",
      "Train Epoch: 8 [015/030], Loss: 0.007304\n",
      "Train Epoch: 8 [016/030], Loss: 0.006295\n",
      "Train Epoch: 8 [017/030], Loss: -0.004139\n",
      "Train Epoch: 8 [018/030], Loss: -0.008658\n",
      "Train Epoch: 8 [019/030], Loss: -0.002083\n",
      "Train Epoch: 8 [020/030], Loss: 0.008660\n",
      "Train Epoch: 8 [021/030], Loss: -0.009848\n",
      "Train Epoch: 8 [022/030], Loss: 0.014116\n",
      "Train Epoch: 8 [023/030], Loss: 0.021403\n",
      "Train Epoch: 8 [024/030], Loss: 0.000626\n",
      "Train Epoch: 8 [025/030], Loss: -0.004241\n",
      "Train Epoch: 8 [026/030], Loss: 0.000349\n",
      "Train Epoch: 8 [027/030], Loss: 0.012241\n",
      "Train Epoch: 8 [028/030], Loss: 0.004562\n",
      "Train Epoch: 8 [029/030], Loss: 0.012499\n",
      "Train Epoch: 8 [030/030], Loss: 0.019618\n",
      "Test set: Average loss: 0.0000, Accuracy: 9892/10000 (98.920%)\n",
      "Train Epoch: 9 [001/030], Loss: -0.001988\n",
      "Train Epoch: 9 [002/030], Loss: 0.003776\n",
      "Train Epoch: 9 [003/030], Loss: 0.015059\n",
      "Train Epoch: 9 [004/030], Loss: -0.001620\n",
      "Train Epoch: 9 [005/030], Loss: -0.019521\n",
      "Train Epoch: 9 [006/030], Loss: 0.002289\n",
      "Train Epoch: 9 [007/030], Loss: 0.012511\n",
      "Train Epoch: 9 [008/030], Loss: -0.007373\n",
      "Train Epoch: 9 [009/030], Loss: 0.018437\n",
      "Train Epoch: 9 [010/030], Loss: -0.017523\n",
      "Train Epoch: 9 [011/030], Loss: -0.005026\n",
      "Train Epoch: 9 [012/030], Loss: -0.001942\n",
      "Train Epoch: 9 [013/030], Loss: -0.001877\n",
      "Train Epoch: 9 [014/030], Loss: -0.009616\n",
      "Train Epoch: 9 [015/030], Loss: 0.000133\n",
      "Train Epoch: 9 [016/030], Loss: 0.265596\n",
      "Train Epoch: 9 [017/030], Loss: -0.000688\n",
      "Train Epoch: 9 [018/030], Loss: -0.016021\n",
      "Train Epoch: 9 [019/030], Loss: -0.017707\n",
      "Train Epoch: 9 [020/030], Loss: 0.323291\n",
      "Train Epoch: 9 [021/030], Loss: 0.008207\n",
      "Train Epoch: 9 [022/030], Loss: -0.019084\n",
      "Train Epoch: 9 [023/030], Loss: 0.024690\n",
      "Train Epoch: 9 [024/030], Loss: -0.016090\n",
      "Train Epoch: 9 [025/030], Loss: -0.010015\n",
      "Train Epoch: 9 [026/030], Loss: 0.000408\n",
      "Train Epoch: 9 [027/030], Loss: 0.003755\n",
      "Train Epoch: 9 [028/030], Loss: -0.021272\n",
      "Train Epoch: 9 [029/030], Loss: -0.017705\n",
      "Train Epoch: 9 [030/030], Loss: -0.024663\n",
      "Test set: Average loss: 0.0000, Accuracy: 9897/10000 (98.970%)\n",
      "Train Epoch: 10 [001/030], Loss: 0.004459\n",
      "Train Epoch: 10 [002/030], Loss: -0.031147\n",
      "Train Epoch: 10 [003/030], Loss: -0.014101\n",
      "Train Epoch: 10 [004/030], Loss: -0.015606\n",
      "Train Epoch: 10 [005/030], Loss: -0.008630\n",
      "Train Epoch: 10 [006/030], Loss: 0.020468\n",
      "Train Epoch: 10 [007/030], Loss: -0.018823\n",
      "Train Epoch: 10 [008/030], Loss: -0.011599\n",
      "Train Epoch: 10 [009/030], Loss: -0.014328\n",
      "Train Epoch: 10 [010/030], Loss: -0.009885\n",
      "Train Epoch: 10 [011/030], Loss: 0.101745\n",
      "Train Epoch: 10 [012/030], Loss: 0.008614\n",
      "Train Epoch: 10 [013/030], Loss: -0.001963\n",
      "Train Epoch: 10 [014/030], Loss: 0.010577\n",
      "Train Epoch: 10 [015/030], Loss: -0.011383\n",
      "Train Epoch: 10 [016/030], Loss: -0.014924\n",
      "Train Epoch: 10 [017/030], Loss: -0.028245\n",
      "Train Epoch: 10 [018/030], Loss: -0.026911\n",
      "Train Epoch: 10 [019/030], Loss: 0.011885\n",
      "Train Epoch: 10 [020/030], Loss: -0.031077\n",
      "Train Epoch: 10 [021/030], Loss: -0.005739\n",
      "Train Epoch: 10 [022/030], Loss: -0.010905\n",
      "Train Epoch: 10 [023/030], Loss: -0.014142\n",
      "Train Epoch: 10 [024/030], Loss: 0.000040\n",
      "Train Epoch: 10 [025/030], Loss: -0.003920\n",
      "Train Epoch: 10 [026/030], Loss: 0.013820\n",
      "Train Epoch: 10 [027/030], Loss: 0.011472\n",
      "Train Epoch: 10 [028/030], Loss: -0.017834\n",
      "Train Epoch: 10 [029/030], Loss: 0.015529\n",
      "Train Epoch: 10 [030/030], Loss: -0.017834\n",
      "Test set: Average loss: 0.0000, Accuracy: 9891/10000 (98.910%)\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=2048, shuffle=True, pin_memory=True)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0.\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = -model.marginal_log_likelihood(output, target, n_data=len(train_dataset))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print('Train Epoch: %d [%03d/%03d], Loss: %.6f' % (epoch, batch_idx + 1, len(train_loader), loss.data[0]))\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    feature_extractor.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        output = model(data)\n",
    "        pred = output.argmax()\n",
    "        correct += pred.eq(target.view_as(pred)).data.cpu().sum()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.3f}%)'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "n_epochs = 10\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    train(epoch)\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gpleiss/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:24: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 0.0000, Accuracy: 9896/10000 (98.960%)\n"
     ]
    }
   ],
   "source": [
    "test()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
