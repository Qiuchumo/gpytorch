{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gpytorch\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torchvision import transforms\n",
    "\n",
    "gpytorch.functions.use_toeplitz = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ways = 5\n",
    "shots = 1\n",
    "train_dir_str = \"way%dshot%d\" %(ways, shots)\n",
    "test_dir_str = \"way%dtest\" %ways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gp346/anaconda2/lib/python2.7/site-packages/torchvision-0.2.0-py2.7.egg/torchvision/transforms/transforms.py:156: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\ntest_mnist = torchvision.datasets.ImageFolder('/tmp', split='test',\\n                                        download=True, transform=transforms.Compose([\\n                       transforms.ToTensor()\\n                   ]))\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_base_omni = torchvision.datasets.ImageFolder('/scratch/bw462/omni_data/general', transform=transforms.Compose([\n",
    "                        transforms.Scale((28,28)),\n",
    "                        transforms.ToTensor()\n",
    "                   ]))                                              \n",
    "\"\"\"\n",
    "test_mnist = torchvision.datasets.ImageFolder('/tmp', split='test',\n",
    "                                        download=True, transform=transforms.Compose([\n",
    "                       transforms.ToTensor()\n",
    "                   ]))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FeatureExtractor(nn.Sequential):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(FeatureExtractor, self).__init__(nn.Conv2d(1, 32, kernel_size=5, padding=2),\n",
    "                                 nn.BatchNorm2d(32),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.MaxPool2d(2, 2),\n",
    "                                 nn.Conv2d(32, 64, kernel_size=5, padding=2),\n",
    "                                 nn.BatchNorm2d(64),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.MaxPool2d(2, 2))\n",
    "        \n",
    "class Bottleneck(nn.Sequential):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Bottleneck, self).__init__(nn.Linear(64*7*7, 128),\n",
    "                                         nn.BatchNorm1d(128),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Linear(128, 128),\n",
    "                                 nn.BatchNorm1d(128),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Linear(128,64),\n",
    "                                 nn.BatchNorm1d(64))\n",
    "\n",
    "class LeNet(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.feature_extractor = FeatureExtractor()\n",
    "        self.bottleneck = Bottleneck()\n",
    "        self.final_layer = nn.Sequential(\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Linear(64,1319))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        input_x = x[:,0,:,:].unsqueeze(1)\n",
    "        features = self.feature_extractor(input_x)\n",
    "        bottlenecked_features = self.bottleneck(features.view(-1, 64 * 7 * 7))\n",
    "        classification = self.final_layer(bottlenecked_features)\n",
    "        return classification\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data_loader = torch.utils.data.DataLoader(train_base_omni, shuffle=True, pin_memory=True, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = LeNet().cuda() \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 0\n",
    "if num_epochs > 0:\n",
    "    model.train()\n",
    "    for i in range(num_epochs):\n",
    "        for x, y in train_data_loader:\n",
    "            optimizer.zero_grad()\n",
    "            x = Variable(x).cuda()\n",
    "            y = Variable(y).cuda()\n",
    "            output = model(x)\n",
    "            loss = criterion(output, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(\"Loss: %.3f\" % loss.data[0])\n",
    "    torch.save(model.state_dict(), '/scratch/bw462/omni_net.dat')\n",
    "else:\n",
    "    model.load_state_dict(torch.load('/scratch/bw462/omni_net.dat'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9992\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "#test_data_loader = torch.utils.data.DataLoader(test_mnist, shuffle=False, pin_memory=True, batch_size=256)\n",
    "avg = 0.\n",
    "i = 0.\n",
    "for test_batch_x, test_batch_y in train_data_loader:\n",
    "    predictions = model(Variable(test_batch_x).cuda()).max(-1)[1]\n",
    "    test_batch_y = Variable(test_batch_y).cuda()\n",
    "    avg += torch.eq(predictions, test_batch_y).float().mean().data[0]\n",
    "    i += 1.\n",
    "print('Accuracy: %.4f' % (avg / i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list(model.bottleneck.modules())[-1].weight.data.fill_(1)\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gpytorch.kernels import RBFKernel, GridInterpolationKernel\n",
    "\n",
    "class DeepKernel(gpytorch.Module):\n",
    "    def __init__(self, model):\n",
    "        super(DeepKernel, self).__init__()\n",
    "        self.feature_extractor = model.feature_extractor\n",
    "        self.bottleneck = model.bottleneck\n",
    "        self.gp_layer = GPLayer()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        features = self.feature_extractor(x[:,0,:,:].unsqueeze(1))\n",
    "        bottlenecked_features = self.bottleneck(features.view(-1, 64 * 7 * 7))\n",
    "        gp_output = self.gp_layer(0.1*bottlenecked_features)\n",
    "        return gp_output\n",
    "    \n",
    "    \n",
    "class LatentFunction(gpytorch.AdditiveGridInducingPointModule):\n",
    "    def __init__(self):\n",
    "        super(LatentFunction, self).__init__(grid_size=256, grid_bounds=[(-7, 7)],\n",
    "                                             n_components=64, mixing_params=False, sum_output=False)\n",
    "        cov_module = RBFKernel()\n",
    "        cov_module.initialize(log_lengthscale=2)\n",
    "        self.cov_module = cov_module\n",
    "        \n",
    "    def forward(self, x):\n",
    "        mean = Variable(x.data.new(len(x)).zero_())\n",
    "        covar = self.cov_module(x)\n",
    "        return gpytorch.random_variables.GaussianRandomVariable(mean, covar)\n",
    "\n",
    "    \n",
    "class GPLayer(gpytorch.GPModel):\n",
    "    def __init__(self, n_dims=64):\n",
    "        super(GPLayer, self).__init__(gpytorch.likelihoods.SoftmaxLikelihood(n_features=64, n_classes=1319))\n",
    "        self.latent_function = LatentFunction()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        res = self.latent_function(x)\n",
    "        return res\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "deep_kernel = DeepKernel(model).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gp346/gpytorch/gpytorch/likelihoods/softmax_likelihood.py:46: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  softmax = nn.functional.softmax(mixed_fs.t()).view(n_data, n_samples, self.n_classes)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score\n",
      "0.999577702703\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeepKernel(\n",
       "  (feature_extractor): FeatureExtractor(\n",
       "    (0): Conv2d (1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (2): ReLU()\n",
       "    (3): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
       "    (4): Conv2d (32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (6): ReLU()\n",
       "    (7): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
       "  )\n",
       "  (bottleneck): Bottleneck(\n",
       "    (0): Linear(in_features=3136, out_features=128)\n",
       "    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (2): ReLU()\n",
       "    (3): Linear(in_features=128, out_features=128)\n",
       "    (4): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (5): ReLU()\n",
       "    (6): Linear(in_features=128, out_features=64)\n",
       "    (7): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "  )\n",
       "  (gp_layer): GPLayer(\n",
       "    (likelihood): SoftmaxLikelihood(\n",
       "    )\n",
       "    (latent_function): LatentFunction(\n",
       "      (cov_module): GridInterpolationKernel(\n",
       "        (base_kernel_module): RBFKernel(\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find optimal model hyperparameters\n",
    "optimizer = torch.optim.Adam(deep_kernel.gp_layer.parameters(), lr=0.01)\n",
    "optimizer.n_iter = 0\n",
    "num_epochs = 0\n",
    "if num_epochs > 0:\n",
    "    deep_kernel.train()\n",
    "    for i in range(num_epochs):\n",
    "        for j, (train_x_batch, train_y_batch) in enumerate(train_data_loader):\n",
    "            train_x_batch = Variable(train_x_batch).cuda()\n",
    "            train_y_batch = Variable(train_y_batch).cuda()\n",
    "            optimizer.zero_grad()\n",
    "            output = deep_kernel(train_x_batch)\n",
    "            loss = -deep_kernel.gp_layer.marginal_log_likelihood(output, train_y_batch, n_data=len(train_base_omni))\n",
    "            loss.backward()\n",
    "            optimizer.n_iter += 1\n",
    "            print('Iter %d/%d - Loss: %.3f' % (\n",
    "                i + 1, num_epochs, loss.data[0],\n",
    "            ))\n",
    "            optimizer.step()\n",
    "    torch.save(deep_kernel.state_dict(), '/scratch/bw462/omni_gp.dat')\n",
    "else:\n",
    "    deep_kernel.load_state_dict(torch.load('/scratch/bw462/omni_gp.dat'))\n",
    "    \n",
    "    \n",
    "deep_kernel.eval()\n",
    "avg = 0.\n",
    "i = 0.\n",
    "for test_batch_x, test_batch_y in train_data_loader:\n",
    "    predictions = deep_kernel(Variable(test_batch_x).cuda()).argmax()\n",
    "    test_batch_y = Variable(test_batch_y).cuda()\n",
    "    avg += torch.eq(predictions, test_batch_y).float().mean().data[0]\n",
    "    i += 1.\n",
    "\n",
    "print('Score')\n",
    "print(avg / i)\n",
    "deep_kernel.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9996\n"
     ]
    }
   ],
   "source": [
    "deep_kernel.eval()\n",
    "for test_batch_x, test_batch_y in train_data_loader:\n",
    "    predictions = deep_kernel(Variable(test_batch_x).cuda()).representation().max(-1)[1]\n",
    "    test_batch_y = Variable(test_batch_y).cuda()\n",
    "    avg += torch.eq(predictions, test_batch_y).float().mean().data[0]\n",
    "    i += 1.\n",
    "print('Accuracy: %.4f' % (avg / i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_shots_omni = torchvision.datasets.ImageFolder('/scratch/bw462/omni_data/' + train_dir_str, transform=transforms.Compose([\n",
    "                        transforms.Scale((28,28)),\n",
    "                        transforms.ToTensor()\n",
    "                   ]))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "oneshot_model = DeepKernel(model).cuda()\n",
    "oneshot_model.load_state_dict(deep_kernel.state_dict())\n",
    "oneshot_model.gp_layer.likelihood = gpytorch.likelihoods.SoftmaxLikelihood(n_features=64, n_classes=1319).cuda()\n",
    "oneshot_model.gp_layer.likelihood.mixing_weights.data.copy_(deep_kernel.gp_layer.likelihood.mixing_weights.data)\n",
    "shots_loader = torch.utils.data.DataLoader(train_shots_omni, batch_size=512., pin_memory=True, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1/500 - Loss: -653.641\n",
      "Iter 2/500 - Loss: -451.821\n",
      "Iter 3/500 - Loss: -478.875\n",
      "Iter 4/500 - Loss: -532.491\n",
      "Iter 5/500 - Loss: -555.698\n",
      "Iter 6/500 - Loss: -710.067\n",
      "Iter 7/500 - Loss: -577.127\n",
      "Iter 8/500 - Loss: -607.021\n",
      "Iter 9/500 - Loss: -470.180\n",
      "Iter 10/500 - Loss: -883.101\n",
      "Iter 11/500 - Loss: -539.354\n",
      "Iter 12/500 - Loss: -858.064\n",
      "Iter 13/500 - Loss: -969.110\n",
      "Iter 14/500 - Loss: -550.667\n",
      "Iter 15/500 - Loss: -718.060\n",
      "Iter 16/500 - Loss: -304.329\n",
      "Iter 17/500 - Loss: -303.624\n",
      "Iter 18/500 - Loss: -381.021\n",
      "Iter 19/500 - Loss: -793.212\n",
      "Iter 20/500 - Loss: -714.041\n",
      "Iter 21/500 - Loss: -415.037\n",
      "Iter 22/500 - Loss: -695.110\n",
      "Iter 23/500 - Loss: -701.122\n",
      "Iter 24/500 - Loss: -320.343\n",
      "Iter 25/500 - Loss: -659.692\n",
      "Iter 26/500 - Loss: -761.248\n",
      "Iter 27/500 - Loss: -656.047\n",
      "Iter 28/500 - Loss: -330.620\n",
      "Iter 29/500 - Loss: -441.531\n",
      "Iter 30/500 - Loss: -458.894\n",
      "Iter 31/500 - Loss: -533.302\n",
      "Iter 32/500 - Loss: -320.048\n",
      "Iter 33/500 - Loss: -683.735\n",
      "Iter 34/500 - Loss: -155.642\n",
      "Iter 35/500 - Loss: -680.333\n",
      "Iter 36/500 - Loss: -516.334\n",
      "Iter 37/500 - Loss: -615.521\n",
      "Iter 38/500 - Loss: -545.816\n",
      "Iter 39/500 - Loss: -215.711\n",
      "Iter 40/500 - Loss: -817.123\n",
      "Iter 41/500 - Loss: -455.070\n",
      "Iter 42/500 - Loss: -723.524\n",
      "Iter 43/500 - Loss: -651.610\n",
      "Iter 44/500 - Loss: -748.673\n",
      "Iter 45/500 - Loss: -546.884\n",
      "Iter 46/500 - Loss: -172.290\n",
      "Iter 47/500 - Loss: -462.091\n",
      "Iter 48/500 - Loss: -311.007\n",
      "Iter 49/500 - Loss: -734.214\n",
      "Iter 50/500 - Loss: -383.379\n",
      "Iter 51/500 - Loss: -670.951\n",
      "Iter 52/500 - Loss: -440.372\n",
      "Iter 53/500 - Loss: -595.431\n",
      "Iter 54/500 - Loss: -322.136\n",
      "Iter 55/500 - Loss: -747.957\n",
      "Iter 56/500 - Loss: -692.372\n",
      "Iter 57/500 - Loss: -482.837\n",
      "Iter 58/500 - Loss: -432.800\n",
      "Iter 59/500 - Loss: -368.037\n",
      "Iter 60/500 - Loss: -461.925\n",
      "Iter 61/500 - Loss: -890.953\n",
      "Iter 62/500 - Loss: -1088.172\n",
      "Iter 63/500 - Loss: -684.716\n",
      "Iter 64/500 - Loss: -871.341\n",
      "Iter 65/500 - Loss: -742.209\n",
      "Iter 66/500 - Loss: -534.799\n",
      "Iter 67/500 - Loss: -570.078\n",
      "Iter 68/500 - Loss: -863.932\n",
      "Iter 69/500 - Loss: -643.166\n",
      "Iter 70/500 - Loss: -530.022\n",
      "Iter 71/500 - Loss: -609.286\n",
      "Iter 72/500 - Loss: -726.644\n",
      "Iter 73/500 - Loss: -405.833\n",
      "Iter 74/500 - Loss: -882.195\n",
      "Iter 75/500 - Loss: -610.297\n",
      "Iter 76/500 - Loss: -388.316\n",
      "Iter 77/500 - Loss: -589.362\n",
      "Iter 78/500 - Loss: -321.468\n",
      "Iter 79/500 - Loss: -556.126\n",
      "Iter 80/500 - Loss: -625.869\n",
      "Iter 81/500 - Loss: -670.933\n",
      "Iter 82/500 - Loss: -680.915\n",
      "Iter 83/500 - Loss: -182.925\n",
      "Iter 84/500 - Loss: -402.100\n",
      "Iter 85/500 - Loss: -869.385\n",
      "Iter 86/500 - Loss: -292.377\n",
      "Iter 87/500 - Loss: -564.725\n",
      "Iter 88/500 - Loss: -440.874\n",
      "Iter 89/500 - Loss: -667.574\n",
      "Iter 90/500 - Loss: -670.470\n",
      "Iter 91/500 - Loss: -568.772\n",
      "Iter 92/500 - Loss: -246.607\n",
      "Iter 93/500 - Loss: -748.617\n",
      "Iter 94/500 - Loss: -913.081\n",
      "Iter 95/500 - Loss: -580.540\n",
      "Iter 96/500 - Loss: -498.918\n",
      "Iter 97/500 - Loss: -658.324\n",
      "Iter 98/500 - Loss: -805.309\n",
      "Iter 99/500 - Loss: -192.631\n",
      "Iter 100/500 - Loss: -345.794\n",
      "Iter 101/500 - Loss: -715.626\n",
      "Iter 102/500 - Loss: -142.770\n",
      "Iter 103/500 - Loss: -754.270\n",
      "Iter 104/500 - Loss: -321.902\n",
      "Iter 105/500 - Loss: -551.874\n",
      "Iter 106/500 - Loss: -548.675\n",
      "Iter 107/500 - Loss: -549.519\n",
      "Iter 108/500 - Loss: -367.560\n",
      "Iter 109/500 - Loss: -570.183\n",
      "Iter 110/500 - Loss: -415.936\n",
      "Iter 111/500 - Loss: -683.332\n",
      "Iter 112/500 - Loss: -394.336\n",
      "Iter 113/500 - Loss: -393.628\n",
      "Iter 114/500 - Loss: -389.764\n",
      "Iter 115/500 - Loss: -410.417\n",
      "Iter 116/500 - Loss: -696.621\n",
      "Iter 117/500 - Loss: -698.021\n",
      "Iter 118/500 - Loss: -525.634\n",
      "Iter 119/500 - Loss: -337.787\n",
      "Iter 120/500 - Loss: -484.129\n",
      "Iter 121/500 - Loss: -641.230\n",
      "Iter 122/500 - Loss: -782.466\n",
      "Iter 123/500 - Loss: -481.971\n",
      "Iter 124/500 - Loss: -647.980\n",
      "Iter 125/500 - Loss: -347.424\n",
      "Iter 126/500 - Loss: -682.877\n",
      "Iter 127/500 - Loss: -481.943\n",
      "Iter 128/500 - Loss: -613.954\n",
      "Iter 129/500 - Loss: -387.029\n",
      "Iter 130/500 - Loss: -143.093\n",
      "Iter 131/500 - Loss: -528.778\n",
      "Iter 132/500 - Loss: -442.678\n",
      "Iter 133/500 - Loss: -725.524\n",
      "Iter 134/500 - Loss: -513.818\n",
      "Iter 135/500 - Loss: -836.245\n",
      "Iter 136/500 - Loss: -684.562\n",
      "Iter 137/500 - Loss: -767.284\n",
      "Iter 138/500 - Loss: -366.575\n",
      "Iter 139/500 - Loss: -469.895\n",
      "Iter 140/500 - Loss: -711.965\n",
      "Iter 141/500 - Loss: -389.832\n",
      "Iter 142/500 - Loss: -614.465\n",
      "Iter 143/500 - Loss: -784.608\n",
      "Iter 144/500 - Loss: -579.477\n",
      "Iter 145/500 - Loss: -445.592\n",
      "Iter 146/500 - Loss: -852.759\n",
      "Iter 147/500 - Loss: -115.008\n",
      "Iter 148/500 - Loss: -63.532\n",
      "Iter 149/500 - Loss: -206.090\n",
      "Iter 150/500 - Loss: -536.982\n",
      "Iter 151/500 - Loss: -767.323\n",
      "Iter 152/500 - Loss: -59.024\n",
      "Iter 153/500 - Loss: -682.599\n",
      "Iter 154/500 - Loss: -576.289\n",
      "Iter 155/500 - Loss: -578.408\n",
      "Iter 156/500 - Loss: -682.626\n",
      "Iter 157/500 - Loss: -287.746\n",
      "Iter 158/500 - Loss: -507.508\n",
      "Iter 159/500 - Loss: -559.727\n",
      "Iter 160/500 - Loss: -483.862\n",
      "Iter 161/500 - Loss: -259.165\n",
      "Iter 162/500 - Loss: -486.138\n",
      "Iter 163/500 - Loss: -814.513\n",
      "Iter 164/500 - Loss: -359.286\n",
      "Iter 165/500 - Loss: -498.369\n",
      "Iter 166/500 - Loss: -660.828\n",
      "Iter 167/500 - Loss: -814.661\n",
      "Iter 168/500 - Loss: -448.163\n",
      "Iter 169/500 - Loss: -727.962\n",
      "Iter 170/500 - Loss: -678.335\n",
      "Iter 171/500 - Loss: -248.702\n",
      "Iter 172/500 - Loss: -636.567\n",
      "Iter 173/500 - Loss: -656.256\n",
      "Iter 174/500 - Loss: -623.594\n",
      "Iter 175/500 - Loss: -689.080\n",
      "Iter 176/500 - Loss: -663.995\n",
      "Iter 177/500 - Loss: -383.612\n",
      "Iter 178/500 - Loss: -291.003\n",
      "Iter 179/500 - Loss: -702.400\n",
      "Iter 180/500 - Loss: -288.862\n",
      "Iter 181/500 - Loss: -359.250\n",
      "Iter 182/500 - Loss: -655.689\n",
      "Iter 183/500 - Loss: -189.721\n",
      "Iter 184/500 - Loss: -709.715\n",
      "Iter 185/500 - Loss: -745.676\n",
      "Iter 186/500 - Loss: -512.346\n",
      "Iter 187/500 - Loss: -314.841\n",
      "Iter 188/500 - Loss: -718.897\n",
      "Iter 189/500 - Loss: -863.304\n",
      "Iter 190/500 - Loss: -664.153\n",
      "Iter 191/500 - Loss: -530.995\n",
      "Iter 192/500 - Loss: -647.926\n",
      "Iter 193/500 - Loss: -679.822\n",
      "Iter 194/500 - Loss: -256.081\n",
      "Iter 195/500 - Loss: -829.586\n",
      "Iter 196/500 - Loss: -620.028\n",
      "Iter 197/500 - Loss: -473.512\n",
      "Iter 198/500 - Loss: -413.085\n",
      "Iter 199/500 - Loss: -204.766\n",
      "Iter 200/500 - Loss: -756.454\n",
      "Iter 201/500 - Loss: -546.668\n",
      "Iter 202/500 - Loss: -557.426\n",
      "Iter 203/500 - Loss: -665.746\n",
      "Iter 204/500 - Loss: -625.239\n",
      "Iter 205/500 - Loss: -518.111\n",
      "Iter 206/500 - Loss: -578.064\n",
      "Iter 207/500 - Loss: -391.962\n",
      "Iter 208/500 - Loss: -737.198\n",
      "Iter 209/500 - Loss: -599.442\n",
      "Iter 210/500 - Loss: -545.947\n",
      "Iter 211/500 - Loss: -696.810\n",
      "Iter 212/500 - Loss: -543.924\n",
      "Iter 213/500 - Loss: -652.712\n",
      "Iter 214/500 - Loss: -249.252\n",
      "Iter 215/500 - Loss: -467.052\n",
      "Iter 216/500 - Loss: -354.112\n",
      "Iter 217/500 - Loss: -442.197\n",
      "Iter 218/500 - Loss: -512.162\n",
      "Iter 219/500 - Loss: -508.177\n",
      "Iter 220/500 - Loss: -816.396\n",
      "Iter 221/500 - Loss: -431.781\n",
      "Iter 222/500 - Loss: -701.335\n",
      "Iter 223/500 - Loss: -274.576\n",
      "Iter 224/500 - Loss: -597.478\n",
      "Iter 225/500 - Loss: -491.965\n",
      "Iter 226/500 - Loss: -511.998\n",
      "Iter 227/500 - Loss: -675.277\n",
      "Iter 228/500 - Loss: -94.446\n",
      "Iter 229/500 - Loss: -647.953\n",
      "Iter 230/500 - Loss: -839.224\n",
      "Iter 231/500 - Loss: -364.959\n",
      "Iter 232/500 - Loss: -210.000\n",
      "Iter 233/500 - Loss: -589.955\n",
      "Iter 234/500 - Loss: -818.562\n",
      "Iter 235/500 - Loss: -521.800\n",
      "Iter 236/500 - Loss: -503.416\n",
      "Iter 237/500 - Loss: -570.387\n",
      "Iter 238/500 - Loss: -727.376\n",
      "Iter 239/500 - Loss: -584.142\n",
      "Iter 240/500 - Loss: -503.677\n",
      "Iter 241/500 - Loss: -702.495\n",
      "Iter 242/500 - Loss: -554.939\n",
      "Iter 243/500 - Loss: -473.629\n",
      "Iter 244/500 - Loss: -635.106\n",
      "Iter 245/500 - Loss: -830.184\n",
      "Iter 246/500 - Loss: -530.070\n",
      "Iter 247/500 - Loss: -394.608\n",
      "Iter 248/500 - Loss: -294.967\n",
      "Iter 249/500 - Loss: -752.519\n",
      "Iter 250/500 - Loss: -556.400\n",
      "Iter 251/500 - Loss: -335.127\n",
      "Iter 252/500 - Loss: -342.454\n",
      "Iter 253/500 - Loss: -667.795\n",
      "Iter 254/500 - Loss: -682.158\n",
      "Iter 255/500 - Loss: -863.511\n",
      "Iter 256/500 - Loss: -305.744\n",
      "Iter 257/500 - Loss: -716.076\n",
      "Iter 258/500 - Loss: -170.959\n",
      "Iter 259/500 - Loss: -375.583\n",
      "Iter 260/500 - Loss: -646.333\n",
      "Iter 261/500 - Loss: -521.612\n",
      "Iter 262/500 - Loss: -635.411\n",
      "Iter 263/500 - Loss: -292.528\n",
      "Iter 264/500 - Loss: -583.508\n",
      "Iter 265/500 - Loss: -764.979\n",
      "Iter 266/500 - Loss: -603.183\n",
      "Iter 267/500 - Loss: -349.473\n",
      "Iter 268/500 - Loss: -433.507\n",
      "Iter 269/500 - Loss: -637.711\n",
      "Iter 270/500 - Loss: -378.852\n",
      "Iter 271/500 - Loss: -766.656\n",
      "Iter 272/500 - Loss: -436.757\n",
      "Iter 273/500 - Loss: -559.297\n",
      "Iter 274/500 - Loss: -722.307\n",
      "Iter 275/500 - Loss: -898.408\n",
      "Iter 276/500 - Loss: -570.981\n",
      "Iter 277/500 - Loss: -703.976\n",
      "Iter 278/500 - Loss: -428.216\n",
      "Iter 279/500 - Loss: -725.808\n",
      "Iter 280/500 - Loss: -481.537\n",
      "Iter 281/500 - Loss: -415.760\n",
      "Iter 282/500 - Loss: -764.146\n",
      "Iter 283/500 - Loss: -430.658\n",
      "Iter 284/500 - Loss: -543.938\n",
      "Iter 285/500 - Loss: -450.248\n",
      "Iter 286/500 - Loss: -356.237\n",
      "Iter 287/500 - Loss: -392.053\n",
      "Iter 288/500 - Loss: -584.044\n",
      "Iter 289/500 - Loss: -250.104\n",
      "Iter 290/500 - Loss: -721.811\n",
      "Iter 291/500 - Loss: -485.496\n",
      "Iter 292/500 - Loss: -605.674\n",
      "Iter 293/500 - Loss: -538.648\n",
      "Iter 294/500 - Loss: -406.894\n",
      "Iter 295/500 - Loss: -548.454\n",
      "Iter 296/500 - Loss: -607.061\n",
      "Iter 297/500 - Loss: -736.977\n",
      "Iter 298/500 - Loss: -514.923\n",
      "Iter 299/500 - Loss: -253.365\n",
      "Iter 300/500 - Loss: -206.710\n",
      "Iter 301/500 - Loss: -597.300\n",
      "Iter 302/500 - Loss: -411.099\n",
      "Iter 303/500 - Loss: -323.145\n",
      "Iter 304/500 - Loss: -20.341\n",
      "Iter 305/500 - Loss: -463.317\n",
      "Iter 306/500 - Loss: -533.651\n",
      "Iter 307/500 - Loss: -872.541\n",
      "Iter 308/500 - Loss: -475.757\n",
      "Iter 309/500 - Loss: -871.687\n",
      "Iter 310/500 - Loss: -804.192\n",
      "Iter 311/500 - Loss: -466.104\n",
      "Iter 312/500 - Loss: -447.556\n",
      "Iter 313/500 - Loss: -708.421\n",
      "Iter 314/500 - Loss: -359.620\n",
      "Iter 315/500 - Loss: -572.275\n",
      "Iter 316/500 - Loss: -806.538\n",
      "Iter 317/500 - Loss: -210.312\n",
      "Iter 318/500 - Loss: -547.747\n",
      "Iter 319/500 - Loss: -691.901\n",
      "Iter 320/500 - Loss: -501.546\n",
      "Iter 321/500 - Loss: -746.654\n",
      "Iter 322/500 - Loss: -441.410\n",
      "Iter 323/500 - Loss: -193.955\n",
      "Iter 324/500 - Loss: -452.235\n",
      "Iter 325/500 - Loss: -575.339\n",
      "Iter 326/500 - Loss: -769.748\n",
      "Iter 327/500 - Loss: -748.303\n",
      "Iter 328/500 - Loss: -760.390\n",
      "Iter 329/500 - Loss: -596.519\n",
      "Iter 330/500 - Loss: -592.228\n",
      "Iter 331/500 - Loss: -493.727\n",
      "Iter 332/500 - Loss: -490.906\n",
      "Iter 333/500 - Loss: -602.200\n",
      "Iter 334/500 - Loss: -676.093\n",
      "Iter 335/500 - Loss: -150.983\n",
      "Iter 336/500 - Loss: -499.965\n",
      "Iter 337/500 - Loss: -582.371\n",
      "Iter 338/500 - Loss: -500.333\n",
      "Iter 339/500 - Loss: -554.069\n",
      "Iter 340/500 - Loss: -435.568\n",
      "Iter 341/500 - Loss: -711.649\n",
      "Iter 342/500 - Loss: -510.026\n",
      "Iter 343/500 - Loss: -696.505\n",
      "Iter 344/500 - Loss: -636.939\n",
      "Iter 345/500 - Loss: -638.370\n",
      "Iter 346/500 - Loss: -383.139\n",
      "Iter 347/500 - Loss: -563.064\n",
      "Iter 348/500 - Loss: -479.665\n",
      "Iter 349/500 - Loss: -508.829\n",
      "Iter 350/500 - Loss: -132.837\n",
      "Iter 351/500 - Loss: -585.834\n",
      "Iter 352/500 - Loss: -618.490\n",
      "Iter 353/500 - Loss: -410.996\n",
      "Iter 354/500 - Loss: -497.027\n",
      "Iter 355/500 - Loss: -552.984\n",
      "Iter 356/500 - Loss: -405.684\n",
      "Iter 357/500 - Loss: -607.458\n",
      "Iter 358/500 - Loss: -606.149\n",
      "Iter 359/500 - Loss: -555.184\n",
      "Iter 360/500 - Loss: -646.100\n",
      "Iter 361/500 - Loss: -121.757\n",
      "Iter 362/500 - Loss: -525.009\n",
      "Iter 363/500 - Loss: -331.449\n",
      "Iter 364/500 - Loss: -777.898\n",
      "Iter 365/500 - Loss: -681.958\n",
      "Iter 366/500 - Loss: -552.804\n",
      "Iter 367/500 - Loss: -570.256\n",
      "Iter 368/500 - Loss: -553.922\n",
      "Iter 369/500 - Loss: -82.177\n",
      "Iter 370/500 - Loss: -349.583\n",
      "Iter 371/500 - Loss: -885.728\n",
      "Iter 372/500 - Loss: -922.736\n",
      "Iter 373/500 - Loss: -453.343\n",
      "Iter 374/500 - Loss: -541.831\n",
      "Iter 375/500 - Loss: -832.560\n",
      "Iter 376/500 - Loss: -746.617\n",
      "Iter 377/500 - Loss: -572.078\n",
      "Iter 378/500 - Loss: -660.280\n",
      "Iter 379/500 - Loss: -777.284\n",
      "Iter 380/500 - Loss: -435.245\n",
      "Iter 381/500 - Loss: -568.061\n",
      "Iter 382/500 - Loss: -89.404\n",
      "Iter 383/500 - Loss: -329.903\n",
      "Iter 384/500 - Loss: -459.259\n",
      "Iter 385/500 - Loss: -455.793\n",
      "Iter 386/500 - Loss: -668.867\n",
      "Iter 387/500 - Loss: -276.448\n",
      "Iter 388/500 - Loss: -249.491\n",
      "Iter 389/500 - Loss: -475.135\n",
      "Iter 390/500 - Loss: -599.199\n",
      "Iter 391/500 - Loss: -501.842\n",
      "Iter 392/500 - Loss: -182.339\n",
      "Iter 393/500 - Loss: -626.912\n",
      "Iter 394/500 - Loss: -526.195\n",
      "Iter 395/500 - Loss: -425.550\n",
      "Iter 396/500 - Loss: -736.717\n",
      "Iter 397/500 - Loss: -661.843\n",
      "Iter 398/500 - Loss: -363.159\n",
      "Iter 399/500 - Loss: -555.539\n",
      "Iter 400/500 - Loss: -902.083\n",
      "Iter 401/500 - Loss: -811.869\n",
      "Iter 402/500 - Loss: -503.770\n",
      "Iter 403/500 - Loss: -437.153\n",
      "Iter 404/500 - Loss: -857.435\n",
      "Iter 405/500 - Loss: -246.021\n",
      "Iter 406/500 - Loss: -403.059\n",
      "Iter 407/500 - Loss: -209.453\n",
      "Iter 408/500 - Loss: -363.982\n",
      "Iter 409/500 - Loss: -648.178\n",
      "Iter 410/500 - Loss: -534.721\n",
      "Iter 411/500 - Loss: -634.113\n",
      "Iter 412/500 - Loss: -283.650\n",
      "Iter 413/500 - Loss: -679.534\n",
      "Iter 414/500 - Loss: -382.270\n",
      "Iter 415/500 - Loss: -689.369\n",
      "Iter 416/500 - Loss: -532.527\n",
      "Iter 417/500 - Loss: -833.466\n",
      "Iter 418/500 - Loss: -647.559\n",
      "Iter 419/500 - Loss: -630.055\n",
      "Iter 420/500 - Loss: -588.329\n",
      "Iter 421/500 - Loss: -416.890\n",
      "Iter 422/500 - Loss: -630.107\n",
      "Iter 423/500 - Loss: -338.627\n",
      "Iter 424/500 - Loss: -230.501\n",
      "Iter 425/500 - Loss: -486.753\n",
      "Iter 426/500 - Loss: -115.042\n",
      "Iter 427/500 - Loss: -680.148\n",
      "Iter 428/500 - Loss: -502.102\n",
      "Iter 429/500 - Loss: -975.072\n",
      "Iter 430/500 - Loss: -328.514\n",
      "Iter 431/500 - Loss: -638.273\n",
      "Iter 432/500 - Loss: -857.655\n",
      "Iter 433/500 - Loss: -367.909\n",
      "Iter 434/500 - Loss: -347.023\n",
      "Iter 435/500 - Loss: -714.670\n",
      "Iter 436/500 - Loss: -682.410\n",
      "Iter 437/500 - Loss: -396.943\n",
      "Iter 438/500 - Loss: -694.507\n",
      "Iter 439/500 - Loss: -498.263\n",
      "Iter 440/500 - Loss: -444.998\n",
      "Iter 441/500 - Loss: -425.250\n",
      "Iter 442/500 - Loss: -518.896\n",
      "Iter 443/500 - Loss: 283.859\n",
      "Iter 444/500 - Loss: -653.638\n",
      "Iter 445/500 - Loss: -392.412\n",
      "Iter 446/500 - Loss: -813.812\n",
      "Iter 447/500 - Loss: -479.638\n",
      "Iter 448/500 - Loss: -578.588\n",
      "Iter 449/500 - Loss: -423.418\n",
      "Iter 450/500 - Loss: -608.282\n",
      "Iter 451/500 - Loss: -359.133\n",
      "Iter 452/500 - Loss: -595.935\n",
      "Iter 453/500 - Loss: -483.312\n",
      "Iter 454/500 - Loss: -614.455\n",
      "Iter 455/500 - Loss: -395.642\n",
      "Iter 456/500 - Loss: -426.238\n",
      "Iter 457/500 - Loss: -598.866\n",
      "Iter 458/500 - Loss: -680.791\n",
      "Iter 459/500 - Loss: -503.795\n",
      "Iter 460/500 - Loss: -711.978\n",
      "Iter 461/500 - Loss: -402.685\n",
      "Iter 462/500 - Loss: -837.848\n",
      "Iter 463/500 - Loss: -890.817\n",
      "Iter 464/500 - Loss: -696.043\n",
      "Iter 465/500 - Loss: -421.135\n",
      "Iter 466/500 - Loss: -371.222\n",
      "Iter 467/500 - Loss: -175.587\n",
      "Iter 468/500 - Loss: -650.812\n",
      "Iter 469/500 - Loss: -635.472\n",
      "Iter 470/500 - Loss: -818.600\n",
      "Iter 471/500 - Loss: -633.491\n",
      "Iter 472/500 - Loss: -570.284\n",
      "Iter 473/500 - Loss: -463.081\n",
      "Iter 474/500 - Loss: -451.420\n",
      "Iter 475/500 - Loss: -800.474\n",
      "Iter 476/500 - Loss: -230.384\n",
      "Iter 477/500 - Loss: -800.408\n",
      "Iter 478/500 - Loss: -627.008\n",
      "Iter 479/500 - Loss: -663.711\n",
      "Iter 480/500 - Loss: -675.082\n",
      "Iter 481/500 - Loss: -665.470\n",
      "Iter 482/500 - Loss: -723.041\n",
      "Iter 483/500 - Loss: -435.510\n",
      "Iter 484/500 - Loss: -449.880\n",
      "Iter 485/500 - Loss: -562.106\n",
      "Iter 486/500 - Loss: -756.798\n",
      "Iter 487/500 - Loss: -464.690\n",
      "Iter 488/500 - Loss: -438.790\n",
      "Iter 489/500 - Loss: -521.486\n",
      "Iter 490/500 - Loss: -568.701\n",
      "Iter 491/500 - Loss: -651.778\n",
      "Iter 492/500 - Loss: -526.800\n",
      "Iter 493/500 - Loss: -432.873\n",
      "Iter 494/500 - Loss: -697.194\n",
      "Iter 495/500 - Loss: -270.727\n",
      "Iter 496/500 - Loss: -443.615\n",
      "Iter 497/500 - Loss: -526.195\n",
      "Iter 498/500 - Loss: -321.327\n",
      "Iter 499/500 - Loss: -701.241\n",
      "Iter 500/500 - Loss: -267.561\n"
     ]
    }
   ],
   "source": [
    "# Find optimal model hyperparameters\n",
    "oneshot_model.train()\n",
    "oneshot_model.feature_extractor.eval()\n",
    "oneshot_model.bottleneck.eval()\n",
    "\n",
    "optimizer = torch.optim.Adam(oneshot_model.gp_layer.likelihood.parameters(), lr=0.01)\n",
    "optimizer.n_iter = 0\n",
    "num_epochs = 500\n",
    "for i in range(num_epochs):\n",
    "    for j, (train_x_batch, train_y_batch) in enumerate(shots_loader):\n",
    "        train_x_batch = Variable(train_x_batch).cuda()\n",
    "        train_y_batch = Variable(train_y_batch).cuda()\n",
    "        optimizer.zero_grad()\n",
    "        output = oneshot_model(train_x_batch)\n",
    "        loss = -oneshot_model.gp_layer.marginal_log_likelihood(output, train_y_batch, n_data=len(train_shots_omni))\n",
    "        #kl = oneshot_model.gp_layer.likelihood.kl_div() / len(train_mnist)\n",
    "        #loss = loss + kl\n",
    "        loss.backward()\n",
    "        optimizer.n_iter += 1\n",
    "        print('Iter %d/%d - Loss: %.3f' % (\n",
    "            i + 1, num_epochs, loss.data[0],\n",
    "        ))\n",
    "        optimizer.step()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score\n",
      "0.642857134342\n",
      "Average Confidence\n",
      "0.672204375267\n"
     ]
    }
   ],
   "source": [
    "oneshot_model.eval()\n",
    "test_shots_omni = torchvision.datasets.ImageFolder('/scratch/bw462/omni_data/' + test_dir_str, transform=transforms.Compose([\n",
    "                        transforms.Scale((28,28)),\n",
    "                        transforms.ToTensor()\n",
    "                   ]))    \n",
    "test_shots_loader = torch.utils.data.DataLoader(test_shots_omni, batch_size=512., pin_memory=True, shuffle=True)\n",
    "avg = 0.\n",
    "summed_confidence = 0.\n",
    "i = 0.\n",
    "for test_batch_x, test_batch_y in test_shots_loader:\n",
    "    model_output = oneshot_model(Variable(test_batch_x).cuda())\n",
    "    predictions = model_output.argmax()\n",
    "    confidences = model_output.mass_function.max(-1)[0]\n",
    "    avg_confidence = confidences.mean()\n",
    "    summed_confidence += avg_confidence\n",
    "    test_batch_y = Variable(test_batch_y).cuda()\n",
    "    avg += torch.eq(predictions, test_batch_y).float().mean().data[0]\n",
    "    i += 1.\n",
    "\n",
    "#for pred, y in zip(predictions.data, test_batch_y.data):\n",
    "#    print(pred, y)\n",
    "print('Score')\n",
    "print(avg / i)\n",
    "print('Average Confidence')\n",
    "print(summed_confidence.data[0] / i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 5 way 1 shot  64.3% accuracy 67.2% confidence\n",
    "# 5 way 5 shot  95.7% accuracy 94.1% confidence\n",
    "# 20 way 1 shot 62.1% accuracy 68.4% confidence\n",
    "# 20 way 5 shot 92.1% accuracy 88.8% confidence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
